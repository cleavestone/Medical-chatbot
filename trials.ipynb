{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader,PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone-client[grpc] in c:\\users\\hp\\desktop\\medical-chatbot-llama2\\mchatbot\\lib\\site-packages (4.1.1)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\hp\\desktop\\medical-chatbot-llama2\\mchatbot\\lib\\site-packages (from pinecone-client[grpc]) (2024.6.2)\n",
      "Collecting googleapis-common-protos>=1.53.0 (from pinecone-client[grpc])\n",
      "  Using cached googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting grpcio>=1.59.0 (from pinecone-client[grpc])\n",
      "  Using cached grpcio-1.64.1-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting lz4>=3.1.3 (from pinecone-client[grpc])\n",
      "  Using cached lz4-4.3.3-cp311-cp311-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in c:\\users\\hp\\desktop\\medical-chatbot-llama2\\mchatbot\\lib\\site-packages (from pinecone-client[grpc]) (0.0.7)\n",
      "Collecting protobuf<5.0,>=4.25 (from pinecone-client[grpc])\n",
      "  Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting protoc-gen-openapiv2<0.0.2,>=0.0.1 (from pinecone-client[grpc])\n",
      "  Using cached protoc_gen_openapiv2-0.0.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in c:\\users\\hp\\desktop\\medical-chatbot-llama2\\mchatbot\\lib\\site-packages (from pinecone-client[grpc]) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\hp\\desktop\\medical-chatbot-llama2\\mchatbot\\lib\\site-packages (from pinecone-client[grpc]) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\hp\\desktop\\medical-chatbot-llama2\\mchatbot\\lib\\site-packages (from pinecone-client[grpc]) (2.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\desktop\\medical-chatbot-llama2\\mchatbot\\lib\\site-packages (from tqdm>=4.64.1->pinecone-client[grpc]) (0.4.6)\n",
      "Using cached googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
      "Using cached grpcio-1.64.1-cp311-cp311-win_amd64.whl (4.1 MB)\n",
      "Using cached lz4-4.3.3-cp311-cp311-win_amd64.whl (99 kB)\n",
      "Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Using cached protoc_gen_openapiv2-0.0.1-py3-none-any.whl (7.9 kB)\n",
      "Installing collected packages: protobuf, lz4, grpcio, googleapis-common-protos, protoc-gen-openapiv2\n",
      "Successfully installed googleapis-common-protos-1.63.2 grpcio-1.64.1 lz4-4.3.3 protobuf-4.25.3 protoc-gen-openapiv2-0.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pinecone-client[grpc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connect to Pinecone server\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key='4f1612af-8adc-45aa-ae26-89fab99d7bb7')\n",
    "\n",
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        ) \n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(directory):\n",
    "    loader=PyPDFDirectoryLoader(directory,glob='*.pdf')\n",
    "    data=loader.load()\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "    documents=text_splitter.split_documents(data)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
